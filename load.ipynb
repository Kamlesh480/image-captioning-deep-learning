{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"load.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"aTE1eidZsSuN","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vnWhNJC8M5d5","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","from torchvision import transforms, utils\n","\n","import torchvision.models as models\n","\n","\n","from skimage import io, transform\n","\n","import matplotlib.pyplot as plt # for plotting\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sys\n","import os\n","from PIL import Image\n","import numpy\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')\n","from collections import Counter, defaultdict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kC9JUlQMM5d9"},"source":["### Image Transforms"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g6ClqmTUM5d9","colab":{}},"source":["class Rescale(object):\n","    \"\"\"Rescale the image in a sample to a given size.\n","\n","    Args:\n","        output_size (tuple or int): Desired output size. If tuple, output is\n","            matched to output_size. If int, smaller of image edges is matched\n","            to output_size keeping aspect ratio the same.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, image):\n","        h, w = image.shape[:2]\n","        if isinstance(self.output_size, int):\n","            if h > w:\n","                new_h, new_w = self.output_size * h / w, self.output_size\n","            else:\n","                new_h, new_w = self.output_size, self.output_size * w / h\n","        else:\n","            new_h, new_w = self.output_size\n","\n","        new_h, new_w = int(new_h), int(new_w)\n","        image = image.astype(np.float32)\n","        img = transform.resize(image, (new_h, new_w))\n","        # print('img : ',type(img))\n","        return img\n","\n","\n","class ToTensor(object):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","\n","    def __call__(self, image):\n","        # swap color axis because\n","        # numpy image: H x W x C\n","        # torch image: C X H X W\n","        image = image.astype(np.float32)\n","        image = image.transpose((2, 0, 1))\n","        return image\n","\n","IMAGE_RESIZE = (224, 224)\n","# Sequentially compose the transforms\n","img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K5CM4cBhM5eA"},"source":["### Captions Preprocessing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rbEQS053M5eA","colab":{}},"source":["class CaptionsPreprocessing:\n","    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n","\n","    Args:\n","        captions_file_path (string): captions tsv file path\n","    \"\"\"\n","    def __init__(self, captions_file_path):\n","        self.captions_file_path = captions_file_path\n","\n","        # Read raw captions\n","        self.raw_captions_dict = self.read_raw_captions()\n","\n","        # Preprocess captions\n","        self.captions_dict = self.process_captions()\n","\n","        # Create vocabulary\n","        self.vocab = self.generate_vocabulary()\n","\n","    def read_raw_captions(self):\n","        \"\"\"\n","        Returns:\n","            Dictionary with raw captions list keyed by image ids (integers)\n","        \"\"\"\n","        captions_dict = {}\n","        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n","            for img_caption_line in f.readlines():\n","                img_captions = img_caption_line.strip().split('\\t')\n","                captions_dict[int(img_captions[0])] = img_captions[1:]\n","\n","        return captions_dict\n","\n","    def process_captions(self):\n","        \"\"\"\n","        Use this function to generate dictionary and other preprocessing on captions\n","        \"\"\"\n","        captions_dict = {}\n","        for key in self.raw_captions_dict:\n","            captions = [[\"<SS>\"] + word_tokenize(caption.lower()) + [\"<ES>\"] for caption in self.raw_captions_dict[key]]\n","            for i,caption in enumerate(captions):\n","                new_key = (key,i)\n","                captions_dict[new_key] = caption\n","        #\n","        return captions_dict\n","\n","    def generate_vocabulary(self):\n","        \"\"\"\n","        Use this function to generate dictionary and other preprocessing on captions\n","        \"\"\"\n","        # all_captions = [caption for captions in self.captions_dict.values() for caption in captions]\n","        all_captions = self.captions_dict.values()\n","        token_counts = Counter([token for caption in all_captions for token in caption])\n","        # <UN> unknown token starting in dict indexed at 0\n","        all_tokens = [\"<UN>\"] + [entry[0] for entry in token_counts.most_common()]\n","        # Generate the vocabulary\n","        vocab = {token:index for index,token in enumerate(all_tokens)}\n","        return vocab\n","            \n","    def captions_transform(self, img_caption):\n","        \"\"\"\n","        Use this function to generate tensor tokens for the text captions\n","        Args:\n","            img_caption_list: List of captions for a particular image\n","        \"\"\"\n","        vocab = self.vocab\n","        # maxlength = len(img_caption_list[0])\n","        # for l in img_caption_list:\n","        #     maxlength = (maxlength <= len(l)) and len(l) or maxlength\n","        tenser = torch.zeros([len(img_caption)],dtype=torch.long,requires_grad=False)\n","        # Generate tensors\n","        for i,token in enumerate(img_caption):\n","            tenser[i] = vocab[token]\n","        return tenser\n","\n","# Set the captions tsv file path\n","CAPTIONS_FILE_PATH = '/content/drive/My Drive/Colab Notebooks/caption data/public_test_captions.tsv'\n","captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tngan5eV9q5B","colab_type":"code","colab":{}},"source":["print(captions_preprocessing_obj.vocab)\n","# captions_preprocessing_obj.captions_transform(captions_preprocessing_obj.captions_dict[(257,3)])\n","print('row caption : ',len(captions_preprocessing_obj.raw_captions_dict))\n","print('total caption : ',len(captions_preprocessing_obj.captions_dict))\n","print('vocab size : ',len(captions_preprocessing_obj.vocab))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lzmf-nlhM5eC"},"source":["### Dataset Class"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bUuWkFPpM5eD","colab":{}},"source":["class ImageCaptionsDataset(Dataset):\n","\n","    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n","        \"\"\"\n","        Args:\n","            img_dir (string): Directory with all the images.\n","            captions_dict: Dictionary with captions list keyed by image ids (integers)\n","            img_transform (callable, optional): Optional transform to be applied\n","                on the image sample.\n","\n","            captions_transform: (callable, optional): Optional transform to be applied\n","                on the caption sample (list).\n","        \"\"\"\n","        self.img_dir = img_dir\n","        self.captions_dict = captions_dict\n","        self.img_transform = img_transform\n","        self.captions_transform = captions_transform\n","\n","        self.image_ids = list(captions_dict.keys())\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","\n","    def __getitem__(self, idx):\n","        key,ind = self.image_ids[idx]    #idx is index\n","        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(key))\n","        image = io.imread(img_name)\n","        caption = self.captions_dict[self.image_ids[idx]]\n","\n","        if self.img_transform:\n","            image = self.img_transform(image)\n","\n","        if self.captions_transform:\n","            caption = self.captions_transform(caption)\n","\n","        sample = {'image': image, 'caption': caption}\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLUv8MX5x0H0","colab_type":"code","colab":{}},"source":["class std_EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        super(std_EncoderCNN, self).__init__()\n","    #now here we gone import pretrend cnn and use tranfer learning ye'ah and i'm gonna chage it. FUCK YOU\n","    # conv layers: (in_channel size, out_channels size, kernel_size, stride, padding)\n","        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n"," \n","        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","\n","        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n","        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        # max pooling (kernel_size, stride)\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # fully conected layers:\n","        self.fc6 = nn.Linear(7*7*512, 4096)\n","        self.fc7 = nn.Linear(4096, 4096)\n","        self.fc8 = nn.Linear(4096, embed_size)\n","\n","    def forward(self, x, training=True):\n","        x = F.relu(self.conv1_1(x))\n","        x = F.relu(self.conv1_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2_1(x))\n","        x = F.relu(self.conv2_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3_1(x))\n","        x = F.relu(self.conv3_2(x))\n","        x = F.relu(self.conv3_3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv4_1(x))\n","        x = F.relu(self.conv4_2(x))\n","        x = F.relu(self.conv4_3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv5_1(x))\n","        x = F.relu(self.conv5_2(x))\n","        x = F.relu(self.conv5_3(x))\n","        x = self.pool(x)\n","        x = x.view(-1, 7 * 7 * 512)\n","        x = F.relu(self.fc6(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = F.relu(self.fc7(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = self.fc8(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3J6C-URRrAGh","colab_type":"code","colab":{}},"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        super(EncoderCNN, self).__init__()\n","    #now here we gone import pretrend cnn and use tranfer learning ye'ah and i'm gonna chage it. FUCK YOU\n","    # conv layers: (in_channel size, out_channels size, kernel_size, stride, padding)\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding = 1)    #222\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding = 1)    #220\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding = 2)   #220\n","        # maxpool alplied (220 -> 110)\n","        self.conv4 = nn.Conv2d(128, 256, kernel_size=5, padding = 1)  #106\n","        self.conv5 = nn.Conv2d(256, 256, kernel_size=5, padding = 2)  #\n","\n","        self.conv6 = nn.Conv2d(256, 512, kernel_size=5, padding = 1)  \n","        self.conv7 = nn.Conv2d(512, 512, kernel_size=5)\n","\n","        self.conv8 = nn.Conv2d(512, 1024, kernel_size=5, padding = 1)\n","        self.conv9 = nn.Conv2d(1024, 1024, kernel_size=5)\n","\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        self.fc1 = nn.Linear(7*7*1024, 4096)\n","        self.fc2 = nn.Linear(4096, embed_size)\n","\n","    def forward(self, x, training=True):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv4(x))\n","        x = F.relu(self.conv5(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv6(x))\n","        x = F.relu(self.conv7(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv8(x))\n","        x = F.relu(self.conv9(x))\n","        x = self.pool(x)\n","        x = x.reshape(1,x.reshape(-1).shape[0])\n","        x = F.relu(self.fc6(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = self.fc8(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8g2bmawZx2qv","colab_type":"code","colab":{}},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super(DecoderRNN, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        # self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, features, captions):\n","        embeddings = self.embed(captions)\n","        embeddings = embeddings.permute(1, 0, 2)\n","        features = features.unsqueeze(0)\n","        embeddings = torch.cat((features, embeddings), dim=0)\n","        hiddens, _ = self.lstm(embeddings)\n","        outputs = self.linear(hiddens)   # 90 X 32 X 4096 -> (2880 X 4096) - (2880 X 1)\n","        return outputs\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"W0pi-EQYM5eF"},"source":["### Model Architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6eaz6MgvM5eG","colab":{}},"source":["class ImageCaptionsNet(nn.Module):\n","    def __init__(self):\n","        super(ImageCaptionsNet, self).__init__()\n","        embed_size = 512\n","        vocab_size = len(captions_preprocessing_obj.vocab)\n","        hidden_size = 2048 # it will be changed to (2/3)*(embed_size + vocab_size)\n","        num_layers = 2\n","        self.encoderCNN = std_EncoderCNN(embed_size)\n","        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","\n","    def forward(self, x):\n","        image_batch, captions_batch = x\n","        features = self.encoderCNN.forward(image_batch)\n","        outputs = self.decoderRNN.forward(features, captions_batch)\n","        return outputs\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MzrG8eW2XwCh","colab_type":"text"},"source":["**Data Set Formation**"]},{"cell_type":"code","metadata":{"id":"RzWSTdwaX7t-","colab_type":"code","colab":{}},"source":["class Sampler(Sampler):\n","    def __init__(self,dataset,batch_size):\n","        self.total_indices = len(dataset)\n","        self.sampler_dict = self.genrate_dict(dataset)\n","        self.batch_size = batch_size\n","        self.batches_list = self.create_batch_lists()\n","    def genrate_dict(self, dataset):\n","        sampler_dict_p = {}\n","        for i in range(len(dataset)):\n","            length = len(dataset[i]['caption'])\n","            if length in sampler_dict_p.keys():\n","                sampler_dict_p[length] = sampler_dict_p[length]+[i]\n","            else:\n","                sampler_dict_p[length] = [i]\n","        return sampler_dict_p\n","    \n","    def __len__(self):\n","        return self.total_indices\n","    \n","    def create_batch_lists(self):\n","        ret_list = []\n","        for key in self.sampler_dict.keys():\n","            batches = [ (self.sampler_dict[key][b:(b + self.batch_size)]) for b in range(0, len(self.sampler_dict[key]), self.batch_size)]\n","            for batch in batches:\n","                ret_list.append(batch)\n","        return ret_list\n","    \n","    def __iter__(self):\n","        for i, batch in enumerate(self.batches_list):\n","            yield batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpMGQA_cyVqG","colab_type":"text"},"source":["**Sampler and Data Loader**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZmqfKZIPM5eI","colab":{}},"source":["%cd /content/drive/My\\ Drive/\n","IMAGE_DIR = '/content/drive/My Drive/Colab Notebooks/public_test_images'\n","\n","# Creating the Dataset\n","train_dataset = ImageCaptionsDataset(\n","    img_dir = IMAGE_DIR, \n","    captions_dict = captions_preprocessing_obj.captions_dict, \n","    img_transform = img_transform,\n","    captions_transform = captions_preprocessing_obj.captions_transform\n",")\n","\n","# Define your hyperparameters\n","NUMBER_OF_EPOCHS = 3\n","LEARNING_RATE = 1e-1\n","BATCH_SIZE = 32\n","NUM_WORKERS = 8\n","\n","my_sampler = Sampler(train_dataset,BATCH_SIZE)\n","# for key in my_sampler.sampler_dict.keys():\n","#     print(key,' : ',my_sampler.sampler_dict[key])\n","\n","train_loader = DataLoader(train_dataset, shuffle=False, batch_sampler = my_sampler, num_workers=NUM_WORKERS)\n","# train_loader = DataLoader(train_dataset, shuffle=False, batch_size = BATCH_SIZE, sampler = sampler_dict, num_workers=NUM_WORKERS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rUQtOQ5JM5eI"},"source":["### Training Loop"]},{"cell_type":"code","metadata":{"id":"SgN9QeRyMj66","colab_type":"code","colab":{}},"source":["model = ImageCaptionsNet()\n","device = torch.device(\"cuda:0\")\n","optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","loss_function = nn.CrossEntropyLoss()\n","print(\"GPU avalible\" , torch.cuda.device_count())\n","is_parallel = False\n","if torch.cuda.device_count() >= 1:\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","    model.to(device)\n","    is_parallel = True\n","\n","for epoch in range(NUMBER_OF_EPOCHS):\n","    for batch_idx, sample in enumerate(train_loader):\n","        model.zero_grad()\n","\n","        image_batch, captions_batch = sample['image'], sample['caption']\n","        print(type(image_batch[0]),image_batch.shape)\n","        print(type(captions_batch),captions_batch.shape)\n","        # If GPU training required\n","        # image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n","        if is_parallel:\n","            image_batch = image_batch.to(device)\n","            captions_batch = captions_batch.to(device)\n","        output_captions = model((image_batch,captions_batch))\n","        output_captions = output_captions[:-1]\n","        captions_batch =  captions_batch.t()\n","        print(output_captions.shape,captions_batch.shape)\n","        loss = loss_function(output_captions.reshape(-1,output_captions.shape[2]), captions_batch.reshape(-1))\n","        print(batch_idx,' | ',loss)\n","        loss.backward(loss)\n","        optimizer.step()\n","\n","    print(\"Iteration: \" + str(epoch + 1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xfPBsuJwCenw","colab_type":"text"},"source":["**Rough work**\n","\n"]},{"cell_type":"code","metadata":{"id":"pbjVOewoDrfu","colab_type":"code","colab":{}},"source":["%cd /content/drive/My\\ Drive/\n","directory='/content/drive/My Drive/Colab Notebooks/public_test_images'\n","l = len(os.listdir(directory))\n","\n","path = os.path.join(directory, \"image_280.jpg\")\n","img = Image.open(path)\n","plt.imshow(img)\n","plt.show()\n","img = np.array(img)\n","print(img.shape)\n","img = img_transform(img)\n","img = img.transpose((1,2,0))\n","plt.imshow(img)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V8JlE0aNw4RS","colab_type":"code","colab":{}},"source":["%cd /content/drive/My\\ Drive/\n","directory='/content/drive/My Drive/Colab Notebooks/public_test_images'\n","\n","path = os.path.join(directory, \"image_280\")\n","img = Image.open(path)\n","img = img_transform(img)\n","plt.imshow(img)\n","plt.show()\n","\n","# process and load images feature vector\n","def process_nDload_images(dir, batch_size, start, end):\n","    \"\"\"\n","    load all the images's feature vector in numpy array indexed by their respective id.\n","    \"\"\"\n","    # loads images belongs to dataset located in directory 'dir'.\n","    image_names = os.listdir(dir)\n","    image_names = np.array(image_names)\n","    feature_vector_size = 4096\n","    \n","    # Pre-allocate input-batch-array for images.\n","    batch = np.zeros(shape=(batch_size,224,224,3), dtype=np.float16)\n","\n","    # Pre-allocate output-batch-feature-array for batch-images\n","    transfer_values = np.zeros(shape=(batch_size,feature_vector_size), dtype=np.float16)\n","\n","    for i, imname in enumerate(image_names[start:end]):\n","        path = os.path.join(directory, imname)\n","        img = load_image(path,size=(224,224))\n","        batch[i] = img\n","    \n","    transfer_values = extract_features(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXPLPBm1bmkN","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}