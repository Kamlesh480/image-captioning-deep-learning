{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNnWEnVXsgFyPEtLJkNt7kd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"EBoOJ0vC0Msp","colab_type":"code","colab":{}},"source":["# input image 224*224*3\n","# feature vector 1*1*2048"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LizYr4iQ2rHS","colab_type":"code","colab":{}},"source":["#we'll use inception model for now!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sj7alhpslPa_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599215618340,"user_tz":-330,"elapsed":1729,"user":{"displayName":"Kamlesh Chhipa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0WV-bJN4oMW4fxrw1T8yg_fhggfhkCjus5bxj=s64","userId":"14650246365291470234"}},"outputId":"61a46185-18ac-48da-8174-7c4d99a225b9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w-80xTIG3K-8","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class EncoderCNN(nn.Module):\n","  def __init__(self, embed_size, train_CNN=False):\n","    super(EncoderCNN, self).__init__()\n","    self.train_CNN = train_CNN\n","    #now here we gone import pretrend cnn and use tranfer learning\n","    self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n","    self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.dropout(0.5)\n","\n","  def forward(self, images):\n","    features = self.inception(images)\n","\n","    #fine tuning\n","    for name, param in self.inception.named_parameters():\n","      if \"fc.weight\" in name or \"fc.bias\" in name:\n","        param.requires_grad = True\n","      else:\n","        param.requires_grad = self.train_CNN\n","    \n","    return self.dropout(self.relu(features))\n","\n","#NOW for RNN\n","class DecoderRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(DecoderRNN, self).__init__() \n","    self.embed = nn.Embedding(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","    self.linear = nn.Linear(hidden_size, vocab_size) #hidden layer is output of privius lstm layer\n","    self.dropout = nn.Dropout(0.5)\n","\n","  def forward(self, features, captions):\n","    embeddings = self.dropout(self.embed(captions))\n","    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0) #concatinate\n","    hiddens, _ = self.lstm(embeddings)\n","    outputs = self.linear(hiddens)\n","    return outputs\n","  \n","\n","\n","  \n","class CNNtoRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(CNNtoCNN, self).__init__()\n","    self.encoderCNN = EncoderCNN(embed_size)\n","    self.decodeRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","\n","  def forward(self, images, captions):\n","    features = self.encoderCNN(images)\n","    outputs = self.decoderRNN(features, captions)\n","    return outputs\n","\n","  def caption_image(self, image, vocabulary, max_length=1000):\n","      result_caption = []\n","\n","      with torch.no_grad():\n","          x = self.encoderCNN(image).unsqueeze(0)\n","          states = None #hidden state for the lstm, in the begining states are none, but leter on it we be change.\n","\n","          for _ in range(max_length):\n","              hiddens, states = self.decoderRNN.lstm(x, states)\n","              output = self.decoderRNN.linear(hiddens.squeeze(0))\n","              predicted =output.argmax(1) \n","\n","              result_caption.append(predicted.item())\n","              x = self.decoderRNN.embed(predicted).unsqueeze(0)\n","     \n","              if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                  break\n","      return [vocabulary.itos[idx] for idx in result_caption]     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6zNWqwB_QP_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"status":"error","timestamp":1599239387125,"user_tz":-330,"elapsed":6717,"user":{"displayName":"Kamlesh Chhipa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0WV-bJN4oMW4fxrw1T8yg_fhggfhkCjus5bxj=s64","userId":"14650246365291470234"}},"outputId":"86f22708-964e-4163-d902-37e408f4b357"},"source":[" #traing the model\n","#!pip install utils\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.tensorboard import SummaryWriter\n","from utils import save_checkpoint, load_checkpoint, print_examples\n","from loader import get_loader\n","#from model import CNNtoRNN\n","\n","def train():\n","  transform = transforms.Compose(\n","      [\n","          transforms.Resize((356, 356)),\n","          transforms.RandomCrop((299, 299)),\n","          transforms.ToTensor(),\n","          transforms.Normalizer((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","      ]\n","  )\n","\n","  train_loader, dataset=get_loader(\n","      root_folder =\"/content/drive/My Drive/Colab Notebooks (1)/public_test_images\",\n","      annotation_file = \"/content/drive/My Drive/Colab Notebooks (1)/caption data/txt_public_test_captions.txt\",\n","      transform = transform,\n","      num_workers = 2,\n","  )\n","\n","  tourch.backends,cudnn.benchmark = True\n","  device = torch.device(\"cuda\" if torch.cuda.is_avaliable() else \"cpu\")\n","  load_model  =False\n","  save_model  =True\n","\n","  #hyperperameter\n","  embed_size = 256\n","  hidden_size = 256\n","  vocab_size = len(dataset.vocab)\n","  num_layers = 1\n","  learning_rate = 3e-4\n","  num_epochs = 100\n","\n","  #tensorboard\n","  writer = SummarWriter(\"path\")\n","  step = 0\n","\n","  #initialize model, loss and all\n","  model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n","  criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  if load_model:\n","    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n","  \n","  model.train()\n","\n","  for epoch in range(num_epochs):\n","      print_examples(model, device, dataset)\n","\n","      if save_model:\n","        checkpoint = {\n","            \"state_dict\" : model.state_dict(),\n","            \"optimizer\" : optimizer.state_dict(),\n","            \"step\":step, \n","        }\n","        save_checkpoint(checkpoint)\n","\n","      for idx, (imgs, caption) in enumerate(train_loader):\n","         imgs = imgs.to(device)\n","         captions = captions.to(device)  \n","\n","         outputs = model(imgs, captions[:-1])\n","         loss = critierion(output.reshape(-1, outputs.shape[2]), caption.reshape(-1))\n","\n","         writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n","         step += 1\n","\n","         optimizer.zero_gard()\n","         loss.backward(loss)\n","         optimizer.step()\n","\n","\n","if __name__ == \"__main__\":\n","  train()\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-203fe6ec8b35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#from model import CNNtoRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"Cd6DROGbGBWy","colab_type":"code","colab":{}},"source":["# Build the neural network, expand on top of nn.Module\n","class Network(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # define layers\n","    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n","    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n","\n","    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n","    self.fc2 = nn.Linear(in_features=120, out_features=60)\n","    self.out = nn.Linear(in_features=60, out_features=10)\n","\n","  # define forward function\n","  def forward(self, t):\n","    # conv 1\n","    t = self.conv1(t)\n","    t = F.relu(t)\n","    t = F.max_pool2d(t, kernel_size=2, stride=2)\n","\n","    # conv 2\n","    t = self.conv2(t)\n","    t = F.relu(t)\n","    t = F.max_pool2d(t, kernel_size=2, stride=2)\n","\n","    # fc1\n","    t = t.reshape(-1, 12*4*4)\n","    t = self.fc1(t)\n","    t = F.relu(t)\n","\n","    # fc2\n","    t = self.fc2(t)\n","    t = F.relu(t)\n","\n","    # output\n","    t = self.out(t)\n","    # don't need softmax here since we'll use cross-entropy as activation.\n","\n","    return t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhRrYxw5_RLM","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","net = Net()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DONmSCKGoFH","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Sfhg38cneD7","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","\n","#batch_size = 500   # Number of samples in each batch\n","#epoch_num = 4      # Number of epochs to train the network\n","#lr = 0.0005        # Learning rate\n","\n","\n","class CHHIPA(nn.Module):\n","    def __init__(self, n_classes):\n","        super(CHHIPA, self).__init__()\n","        # conv layers: (in_channel size, out_channels size, kernel_size, stride, padding)\n","        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","\n","        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","\n","        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n","        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        # max pooling (kernel_size, stride)\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # fully conected layers:\n","        self.fc6 = nn.Linear(7*7*512, 4096)\n","        self.fc7 = nn.Linear(4096, 4096)\n","        self.fc8 = nn.Linear(4096, 1000)\n","\n","    def forward(self, x, training=True):\n","        x = F.relu(self.conv1_1(x))\n","        x = F.relu(self.conv1_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2_1(x))\n","        x = F.relu(self.conv2_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3_1(x))\n","        x = F.relu(self.conv3_2(x))\n","        x = F.relu(self.conv3_3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv4_1(x))\n","        x = F.relu(self.conv4_2(x))\n","        x = F.relu(self.conv4_3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv5_1(x))\n","        x = F.relu(self.conv5_2(x))\n","        x = F.relu(self.conv5_3(x))\n","        x = self.pool(x)\n","        x = x.view(-1, 7 * 7 * 512)\n","        x = F.relu(self.fc6(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = F.relu(self.fc7(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = self.fc8(x)\n","        return x\n","\n","# define the CNN and move the network into GPU\n","#vgg16 = VGG16(10)\n","#vgg16.cuda()\n","#from torchvision import models\n","model = CHHIPA(10)\n","print(model)\n","\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class EncoderCNN(nn.Module):\n","  def __init__(self, embed_size, train_CNN=False):\n","    super(EncoderCNN, self).__init__()\n","    self.train_CNN = train_CNN\n","    #now here we gone import pretrend cnn and use tranfer learning\n","    self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n","    self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.dropout(0.5)\n","\n","  def forward(self, images):\n","    features = self.inception(images)\n","\n","    #fine tuning\n","    for name, param in self.inception.named_parameters():\n","      if \"fc.weight\" in name or \"fc.bias\" in name:\n","        param.requires_grad = True\n","      else:\n","        param.requires_grad = self.train_CNN\n","    \n","    return self.dropout(self.relu(features))\n","\n","#NOW for RNN\n","class DecoderRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(DecoderRNN, self).__init__() \n","    self.embed = nn.Embedding(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","    self.linear = nn.Linear(hidden_size, vocab_size) #hidden layer is output of privius lstm layer\n","    self.dropout = nn.Dropout(0.5)\n","\n","  def forward(self, features, captions):\n","    embeddings = self.dropout(self.embed(captions))\n","    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0) #concatinate\n","    hiddens, _ = self.lstm(embeddings)\n","    outputs = self.linear(hiddens)\n","    return outputs\n","  \n","\n","\n","  \n","class CNNtoRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(CNNtoCNN, self).__init__()\n","    self.encoderCNN = EncoderCNN(embed_size)\n","    self.decodeRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","\n","  def forward(self, images, captions):\n","    features = self.encoderCNN(images)\n","    outputs = self.decoderRNN(features, captions)\n","    return outputs\n","\n","  def caption_image(self, image, vocabulary, max_length=1000):\n","      result_caption = []\n","\n","      with torch.no_grad():\n","          x = self.encoderCNN(image).unsqueeze(0)\n","          states = None #hidden state for the lstm, in the begining states are none, but leter on it we be change.\n","\n","          for _ in range(max_length):\n","              hiddens, states = self.decoderRNN.lstm(x, states)\n","              output = self.decoderRNN.linear(hiddens.squeeze(0))\n","              predicted =output.argmax(1) \n","\n","              result_caption.append(predicted.item())\n","              x = self.decoderRNN.embed(predicted).unsqueeze(0)\n","     \n","              if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                  break\n","      return [vocabulary.itos[idx] for idx in result_caption]     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EIKv1hKYoVXE","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        super(EncoderCNN, self).__init__()\n","    #now here we gone import pretrend cnn and use tranfer learning ye'ah and i'm gonna chage it. FUCK YOU\n","    # conv layers: (in_channel size, out_channels size, kernel_size, stride, padding)\n","        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n"," \n","        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","\n","        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n","        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        # max pooling (kernel_size, stride)\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # fully conected layers:\n","        self.fc6 = nn.Linear(7*7*512, 4096)\n","        self.fc7 = nn.Linear(4096, 4096)\n","        self.fc8 = nn.Linear(4096, 1000)\n","\n","\n","    def forward(self, x, training=True):\n","        x = F.relu(self.conv1_1(x))\n","        x = F.relu(self.conv1_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2_1(x))\n","        x = F.relu(self.conv2_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3_1(x))\n","        x = F.relu(self.conv3_2(x))\n","        x = F.relu(self.conv3_3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv4_1(x))\n","        x = F.relu(self.conv4_2(x))\n","        x = F.relu(self.conv4_3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv5_1(x))\n","        x = F.relu(self.conv5_2(x))\n","        x = F.relu(self.conv5_3(x))\n","        x = self.pool(x)\n","        x = x.view(-1, 7 * 7 * 512)\n","        x = F.relu(self.fc6(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = F.relu(self.fc7(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = self.fc8(x)\n","        return x\n","\n","#NOW for RNN\n","class DecoderRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(DecoderRNN, self).__init__() \n","    self.embed = nn.Embedding(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","    self.linear = nn.Linear(hidden_size, vocab_size) #hidden layer is output of privius lstm layer\n","    self.dropout = nn.Dropout(0.5)\n","\n","  def forward(self, features, captions):\n","    embeddings = self.dropout(self.embed(captions))\n","    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0) #concatinate\n","    hiddens, _ = self.lstm(embeddings)\n","    outputs = self.linear(hiddens)\n","    return outputs\n","  \n","\n","\n","  \n","class CNNtoRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(CNNtoCNN, self).__init__()\n","    self.encoderCNN = EncoderCNN(embed_size)\n","    self.decodeRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","\n","  def forward(self, images, captions):\n","    features = self.encoderCNN(images)\n","    outputs = self.decoderRNN(features, captions)\n","    return outputs\n","\n","  def caption_image(self, image, vocabulary, max_length=1000):\n","      result_caption = []\n","\n","      with torch.no_grad():\n","          x = self.encoderCNN(image).unsqueeze(0)\n","          states = None #hidden state for the lstm, in the begining states are none, but leter on it we be change.\n","\n","          for _ in range(max_length):\n","              hiddens, states = self.decoderRNN.lstm(x, states)\n","              output = self.decoderRNN.linear(hiddens.squeeze(0))\n","              predicted =output.argmax(1) \n","\n","              result_caption.append(predicted.item())\n","              x = self.decoderRNN.embed(predicted).unsqueeze(0)\n","     \n","              if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                  break\n","      return [vocabulary.itos[idx] for idx in result_caption]     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHLTIhS8WMRX","colab_type":"code","colab":{}},"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","#hmmmmmm\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class EncoderCNN(nn.Module):\n","  def __init__(self, embed_size, train_CNN=False):\n","    super(EncoderCNN, self).__init__()\n","    self.train_CNN = train_CNN\n","    #now here we gone import pretrend cnn and use tranfer learning\n","    self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n","    self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.dropout(0.5)\n","\n","  def forward(self, images):\n","    features = self.inception(images)\n","\n","    #fine tuning\n","    for name, param in self.inception.named_parameters():\n","      if \"fc.weight\" in name or \"fc.bias\" in name:\n","        param.requires_grad = True\n","      else:\n","        param.requires_grad = self.train_CNN\n","    \n","    return self.dropout(self.relu(features))\n","\n","#NOW for RNN\n","class DecoderRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(DecoderRNN, self).__init__() \n","    self.embed = nn.Embedding(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","    self.linear = nn.Linear(hidden_size, vocab_size) #hidden layer is output of privius lstm layer\n","    self.dropout = nn.Dropout(0.5)\n","\n","  def forward(self, features, captions):\n","    embeddings = self.dropout(self.embed(captions))\n","    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0) #concatinate\n","    hiddens, _ = self.lstm(embeddings)\n","    outputs = self.linear(hiddens)\n","    return outputs\n","  \n","\n","\n","  \n","class CNNtoRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(CNNtoCNN, self).__init__()\n","    self.encoderCNN = EncoderCNN(embed_size)\n","    self.decodeRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","\n","  def forward(self, images, captions):\n","    features = self.encoderCNN(images)\n","    outputs = self.decoderRNN(features, captions)\n","    return outputs\n","\n","  def caption_image(self, image, vocabulary, max_length=1000):\n","      result_caption = []\n","\n","      with torch.no_grad():\n","          x = self.encoderCNN(image).unsqueeze(0)\n","          states = None #hidden state for the lstm, in the begining states are none, but leter on it we be change.\n","\n","          for _ in range(max_length):\n","              hiddens, states = self.decoderRNN.lstm(x, states)\n","              output = self.decoderRNN.linear(hiddens.squeeze(0))\n","              predicted =output.argmax(1) \n","\n","              result_caption.append(predicted.item())\n","              x = self.decoderRNN.embed(predicted).unsqueeze(0)\n","     \n","              if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                  break\n","      return [vocabulary.itos[idx] for idx in result_caption]     \n","\n","\n","\n","\n","\n","\n","\n","\n","class ImageCaptionsNet(nn.Module):\n","    def __init__(self):\n","    #def __init__(self, input_size, hidden_size):\n","        super(ImageCaptionsNet, self).__init__()\n","\n","        # Define your architecture here\n","\n","    def forward(self, x):\n","        x = image_batch, captions_batch\n","\n","        # Forward Propogation\n","\n","        return captions_batch\n","\n","net = ImageCaptionsNet()\n","\n","# If GPU training is required\n","# net = net.cuda()\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","class ImageEncoder(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(ImageEncoder, self).__init__()\n","        self.out = nn.Linear(input_size, hidden_size)\n","        \n","    def forward(self, inputs):\n","        return self.out(inputs)\n","    \n","class DecoderLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(DecoderLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input, hidden):\n","        output = F.relu(input)\n","        output, hidden = self.lstm(output, hidden)\n","        output = self.out(output)\n","        output = F.log_softmax(output.squeeze())\n","        return output.unsqueeze(0), hidden\n","    \n","encoder = ImageEncoder(input_size=4096, hidden_size=300).cuda()\n","decoder = DecoderLSTM(input_size=len(vocabulary), hidden_size=300, output_size=len(vocabulary)).cuda()\n","\n","\n","\n","\n","\n","\n","\n","class ImageCaptionsNet(nn.Module):\n","    def __init__(self):\n","    #def __init__(self, input_size, hidden_size):\n","        super(ImageCaptionsNet, self).__init__()\n","\n","        # Define your architecture here\n","\n","    def forward(self, x):\n","        x = image_batch, captions_batch\n","\n","        # Forward Propogation\n","\n","        return captions_batch\n","\n","net = ImageCaptionsNet()\n","\n","# If GPU training is required\n","# net = net.cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiwgJhOZC-p3","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class EncoderCNN(nn.Module):\n","  def __init__(self, embed_size, train_CNN=False):\n","    super(EncoderCNN, self).__init__()\n","    self.train_CNN = train_CNN\n","    #now here we gone import pretrend cnn and use tranfer learning\n","    self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n","    self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.Dropout(0.5)\n","\n","  def forward(self, image_batch):\n","    features = self.inception(image_batch)\n","\n","    #fine tuning\n","    for name, param in self.inception.named_parameters():\n","      if \"fc.weight\" in name or \"fc.bias\" in name:\n","        param.requires_grad = True\n","      else:\n","        param.requires_grad = self.train_CNN\n","    \n","    return features\n","\n","#NOW for RNN\n","class DecoderRNN(nn.Module):\n","  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","    super(DecoderRNN, self).__init__() \n","    self.embed = nn.Embedding(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","    self.linear = nn.Linear(hidden_size, vocab_size) #hidden layer is output of privius lstm layer\n","    self.dropout = nn.Dropout(0.5)\n","\n","  def forward(self, features, captions_batch):\n","    embeddings = self.dropout(self.embed(captions_batch))\n","    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0) #concatinate\n","    hiddens, _ = self.lstm(embeddings)\n","    outputs = self.linear(hiddens)\n","    return outputs\n","  \n","class ImageCaptionsNet(nn.Module):\n","  def __init__(self):\n","    embed_size = \n","    hidden_size =  \n","    vocab_size = \n","    num_layers = \n","    super(ImageCaptionsNet, self).__init__()\n","    self.encoderCNN = EncoderCNN(embed_size)\n","    self.decodeRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","\n","  def forward(self,image_batch, captions_batch):\n","    features = self.encoderCNN(image_batch)\n","    outputs = self.decoderRNN(features, captions_batch)\n","    return outputs\n","\n","#   def caption_image(self, image, vocabulary, max_length=1000):\n","#       result_caption = []\n","\n","#       with torch.no_grad():\n","#           x = self.encoderCNN(image).unsqueeze(0)\n","    #       states = None #hidden state for the lstm, in the begining states are none, but leter on it we be change.\n","\n","    #       for _ in range(max_length):\n","    #           hiddens, states = self.decoderRNN.lstm(x, states)\n","    #           output = self.decoderRNN.linear(hiddens.squeeze(0))\n","    #           predicted =output.argmax(1) \n","\n","    #           result_caption.append(predicted.item())\n","    #           x = self.decoderRNN.embed(predicted).unsqueeze(0)\n","     \n","    #           if predicted.item() == 2:\n","    #               break\n","    #   return [vocabulary.itos[idx] for idx in result_caption]     \n","\n","\n","net = ImageCaptionsNet()\n","\n","# If GPU training is required\n","# net = net.cuda()\n","\n","\n","\n","\n","# class ImageCaptionsNet(nn.Module):\n","#     def __init__(self):\n","#     # def __init__(self, input_size, hidden_size):\n","#         super(ImageCaptionsNet, self).__init__()\n","\n","#         # Define your architecture here\n","\n","#     def forward(self, x):\n","#         x = image_batch, captions_batch\n","\n","#         # Forward Propogation\n","\n","#         return captions_batch\n","\n","# net = ImageCaptionsNet()\n","\n","# # If GPU training is required\n","# # net = net.cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hPi32-hojP0n","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUvo74r1jPvr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"status":"ok","timestamp":1599440787480,"user_tz":-330,"elapsed":5679,"user":{"displayName":"Kamlesh Chhipa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0WV-bJN4oMW4fxrw1T8yg_fhggfhkCjus5bxj=s64","userId":"14650246365291470234"}},"outputId":"628c103a-aa4e-4e74-f1e9-a61179f2dae8"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","\n","#batch_size = 500   # Number of samples in each batch\n","#epoch_num = 4      # Number of epochs to train the network\n","#lr = 0.0005        # Learning rate\n","\n","\n","class CHHIPA(nn.Module):\n","    def __init__(self, embed_size):\n","        super(CHHIPA, self).__init__()\n","        # conv layers: (in_channel size, out_channels size, kernel_size, stride, padding)\n","        self.layer_1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n","        self.layer_2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n","        # max pooling\n","        self.layer_3 = nn.Conv2d(32, 128, kernel_size=5, padding=2)\n","        self.layer_4 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n","        # max pooling\n","        self.layer_5 = nn.Conv2d(128, 512, kernel_size=5, padding=2)\n","        self.layer_6 = nn.Conv2d(512, 512, kernel_size=5, padding=2)\n","        # max pooling\n","\n","        self.layer_8 = nn.Conv2d(512, 1024, kernel_size=5, padding=2)\n","        self.layer_9 = nn.Conv2d(1024, 1024, kernel_size=5, padding=2)\n","        self.layer_10 = nn.Conv2d(1024, 1024, kernel_size=5, padding=2)\n","        # max pooling\n","        self.layer_11 = nn.Conv2d(1024, 1024, kernel_size=5, padding=2)\n","        self.layer_12 = nn.Conv2d(1024,1024 , kernel_size=5, padding=2)\n","        # max pooling\n","        \n","        # max pooling (kernel_size, stride)\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # fully conected layers:\n","        self.fc6 = nn.Linear(7*7*1024, 8192)\n","        self.fc7 = nn.Linear(8192, 8192)\n","        self.fc8 = nn.Linear(8192, 4096)\n","        self.fc9 = nn.Linear(4096, embed_size)\n","\n","    def forward(self, x, training=True):\n","        x = F.relu(self.layer_1(x))\n","        x = F.relu(self.layer_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.layer_3(x))\n","        x = F.relu(self.layer_4(x))\n","        x = self.pool(x)\n","        x = F.relu(self.layer_5(x))\n","        x = F.relu(self.layer_6(x))\n","        x = self.pool(x)\n","        x = F.relu(self.layer_8(x))\n","        x = F.relu(self.layer_9(x))\n","        x = F.relu(self.layer_10(x))\n","        x = self.pool(x)\n","        x = F.relu(self.layer_11(x))\n","        x = F.relu(self.layer_12(x))\n","        x = self.pool(x)\n","        x = x.reshape(1,x.reshape(-1).shape[0])\n","        x = F.relu(self.fc6(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = F.relu(self.fc7(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = F.relu(self.fc8(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = self.fc9(x)\n","        return x\n","\n","# define the CNN and move the network into GPU\n","#vgg16 = VGG16(10)\n","#vgg16.cuda()\n","#from torchvision import models\n","model = CHHIPA(1000)\n","print(model)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["CHHIPA(\n","  (layer_1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_3): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_4): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_5): Conv2d(128, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_6): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_8): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_9): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_10): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_11): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (layer_12): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc6): Linear(in_features=50176, out_features=8192, bias=True)\n","  (fc7): Linear(in_features=8192, out_features=8192, bias=True)\n","  (fc8): Linear(in_features=8192, out_features=4096, bias=True)\n","  (fc9): Linear(in_features=4096, out_features=1000, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9d6zNx9YFbiB","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}